{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paper_dict(paper):\n",
    "    \"\"\"\n",
    "    Reads in a research paper and returns a dictionary containing the paper ID, abstract, and main text.\n",
    "    Input: research paper --> JSON file\n",
    "    Output: {paper_id: , abstract: , body_text: } --> dictionary\n",
    "    \"\"\"\n",
    "    paper_dict = {}\n",
    "    abstract = ''\n",
    "    text = ''\n",
    "    \n",
    "    try:  # many papers don't have abstracts\n",
    "        for i in paper['abstract']:\n",
    "            abstract += i['text']\n",
    "    except:\n",
    "        pass\n",
    "    for i in paper['body_text']:\n",
    "        text += i['text']\n",
    "    \n",
    "    paper_dict['paper_id'] = paper['paper_id']\n",
    "    paper_dict['abstract'] = abstract\n",
    "    paper_dict['body_text'] = text\n",
    "    \n",
    "    return paper_dict\n",
    "\n",
    "\n",
    "# data_path = 'C://Users//Binyamin//PythonProjects//NLP//final_project//data//'\n",
    "data_path = 'data'\n",
    "lit = []\n",
    "\n",
    "# Searches recursively through Repo for .json files and creates a list of dictionary from them.\n",
    "pathlist = Path(data_path).glob('**/*.json')\n",
    "for path in pathlist:\n",
    "    path_in_str = str(path)  # because path is object not string\n",
    "    with open(path_in_str) as f:\n",
    "        data = json.load(f)\n",
    "    paper_dict = create_paper_dict(data)\n",
    "    lit.append(paper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "788"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Literature - Text Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It is highly contagious, and severe cases can lead to acute respiratory distress or multiple organ failure [3] . On 11 March 2020, the WHO has made the assessment that COVID-19 can be characterised as a pandemic. As of , in total, 1,391,890 cases of COVID-19 have been recorded, and the death toll has reached 81,478 with a rapid increase of cases in Europe and NorthAmerica.8th April 2020The disease can be confirmed by using the reverse-transcription polymerase chain reaction (RT-PCR) test [4] . While being the gold standard for diagnosis, confirming COVID-19 patients using RT-PCR is time-consuming, and both high false-negative rates and low sensitivities may put hurdles for the presumptive patients to be identified and treated early [3] [5] [6] .As a non-invasive imaging technique, computed tomography (CT) can detect those characteristics, e.g., bilateral patchy shadows or ground glass opacity (GGO), manifested in the COVID-19 infected lung [7] [8] .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit[0]['body_text'][: 963]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collating the body text of all the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = []\n",
    "for paper in lit:\n",
    "    papers.append(paper['body_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(f):\n",
    "    tokenized_data = []\n",
    "\n",
    "    for text in sent_tokenize(f):\n",
    "        sentence = [] \n",
    "        for word in word_tokenize(text): \n",
    "            sentence.append(word.lower()) \n",
    "        tokenized_data.append(sentence)\n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_joined = ' '.join(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_tokenized = get_tokens(papers_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the phrases in the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(model):\n",
    "    keys = list(model.wv.vocab.keys())\n",
    "    phrases = []\n",
    "    for k in keys:\n",
    "        if '_' in k:\n",
    "            print(k)\n",
    "            phrases.append(k)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating single word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Word2Vec models\n",
    "model1 = gensim.models.Word2Vec(papers_tokenized, min_count = 1, size = 100, window = 5) \n",
    "\n",
    "model2 = gensim.models.Word2Vec(papers_tokenized, min_count = 1, size = 100, window = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.26923609e-01  2.61659360e+00 -8.27334225e-02  5.38229465e-01\n",
      " -9.94117439e-01 -3.87442493e+00 -6.96345270e-01  2.77918267e+00\n",
      " -1.40695953e+00  3.95146817e-01  2.23959851e+00 -1.71446490e+00\n",
      "  1.55104208e+00  3.01887274e-01 -3.03804398e+00 -6.25375211e-01\n",
      "  1.09006798e+00 -2.22922826e+00  1.49104849e-01  1.68574703e+00\n",
      " -1.19529569e+00 -3.25266451e-01 -3.45773625e+00  8.88256192e-01\n",
      " -9.33322251e-01  4.60733771e-01  1.68860114e+00  1.47994947e+00\n",
      " -2.06382537e+00 -4.34833765e+00  1.65882063e+00 -1.31434572e+00\n",
      "  3.11345315e+00  1.56149471e+00 -4.20445710e-01  1.32742751e+00\n",
      "  2.06985235e+00  7.85256743e-01 -1.21892646e-01 -1.18910718e+00\n",
      "  2.47400117e+00 -1.39158440e+00 -4.51393515e-01  8.70708764e-01\n",
      " -2.52037072e+00  1.40527308e-01 -7.54242182e-01 -2.34408522e+00\n",
      "  6.07615530e-01 -2.46063724e-01  8.19418073e-01 -2.97807097e-01\n",
      "  3.71279192e+00  8.76003385e-01 -1.49308920e+00  3.20777178e-01\n",
      " -2.76745468e-01  2.13290548e+00 -1.36524343e+00 -4.67133790e-01\n",
      "  1.96110234e-01  1.61062133e+00 -1.63277018e+00 -6.94721341e-01\n",
      " -1.19203246e+00 -3.78887713e-01  3.24028945e+00  1.17451727e+00\n",
      " -1.28011096e+00 -2.01638484e+00 -2.41196370e+00 -1.13758743e+00\n",
      " -2.37280536e+00  4.34489083e-03 -2.97275841e-01  2.11936355e+00\n",
      " -1.43760097e+00  2.61705613e+00 -1.99924719e+00 -3.12408209e+00\n",
      " -1.83204579e+00  3.35349917e+00 -2.72329354e+00  5.25034964e-01\n",
      " -2.78789377e+00  9.77635205e-01  1.37366998e+00 -1.71353006e+00\n",
      "  2.05363274e+00  6.16097808e-01 -3.99127960e+00 -1.36883819e+00\n",
      " -3.87980819e-01 -1.21064329e+00 -1.47722137e+00 -2.92425364e-01\n",
      "  3.37672770e-01 -1.12129200e+00  3.10250974e+00 -1.19620121e+00]\n"
     ]
    }
   ],
   "source": [
    "# Checking the vectors\n",
    "print(model1.wv['covid-19'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.84637946,  0.28372577,  0.04496578,  0.19600813, -0.2470242 ,\n",
       "       -0.40887585,  0.14495395,  0.4954242 , -0.543544  ,  0.12964353,\n",
       "        0.60227126, -0.38009283,  0.5444126 ,  0.0414533 ,  0.15818694,\n",
       "        0.14558883,  0.67090315, -0.6638891 ,  0.05648408,  0.706631  ,\n",
       "        0.03201538,  0.22368635, -0.16347015, -0.12725721, -0.20802411,\n",
       "       -0.10820621, -0.09826882,  0.07805505,  0.23418501, -0.6930578 ,\n",
       "        0.51263225, -0.46163473,  0.48860902,  0.22044551,  0.20175064,\n",
       "        0.6469956 ,  0.41784322,  0.6856917 ,  0.41666463,  0.11608683,\n",
       "        0.26542455, -0.37355825, -0.09831123,  0.9270578 ,  0.1818363 ,\n",
       "       -0.27748898, -0.19454962, -0.25502518, -0.14908303, -0.69140774,\n",
       "        0.30506262,  0.04884483, -0.10744089,  0.21876861,  0.2604155 ,\n",
       "        0.02467573, -0.08131506,  0.08704039,  0.04549044, -0.02001595,\n",
       "        0.08753835, -0.12233245,  0.35407674,  0.27864763, -0.3432078 ,\n",
       "        0.10098751,  0.47464132, -0.20520347, -0.31862095, -0.5321154 ,\n",
       "       -0.22215846, -0.32413617, -0.268476  , -0.18791024, -0.16916433,\n",
       "        0.21874885, -0.18054885,  0.42078772, -1.0168086 , -0.02739931,\n",
       "       -0.02391967,  0.19572194, -0.19444601,  0.44290727, -0.3850289 ,\n",
       "       -0.10679845,  0.62040466,  0.04122429,  0.2500692 ,  1.0842018 ,\n",
       "       -0.27941722, -0.43026504, -0.01709222,  0.24588616,  0.07975596,\n",
       "       -0.4684212 ,  0.60778165, -0.11333666, -0.0994666 ,  0.28935963],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.wv['covid-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sukum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24166737"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.similarity('covid-19', 'contagious')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sukum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51311374"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.similarity('covid-19', 'contagious')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating embeddings for phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_transformer = Phrases(papers_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_model_1 = Word2Vec(bigram_transformer[papers_tokenized], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_model_2 = Word2Vec(bigram_transformer[papers_tokenized], min_count = 1, size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_model_3 = Word2Vec(bigram_transformer[papers_tokenized], min_count = 1, size = 100, window = 5, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [phrase_model_1, phrase_model_2, phrase_model_3]\n",
    "\n",
    "for m in models:\n",
    "    print(m.wv['covid-19'])\n",
    "    print(m.wv['highly_contagious'])\n",
    "    print(m.wv.similarity('covid-19', 'highly_contagious'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[^a-z0-9\\s]', '', sentence)\n",
    "    return re.sub(r'\\s{2,}', ' ', sentence)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [token for token in sentence.split() if token not in STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_phrases(sentences):\n",
    "    phrases = Phrases(sentences,\n",
    "                      min_count=5,\n",
    "                      threshold=7,\n",
    "                      progress_per=1000)\n",
    "    return Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_model = build_phrases(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_bi_grams(phrases_model, sentence):\n",
    "    return ' '.join(phrases_model[sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_bi_grams(n_grams, document):\n",
    "    output = []\n",
    "    for sentence in document:\n",
    "        clean_text = clean(sentence)\n",
    "        tokenized_text = tokenize(clean_text)\n",
    "        parsed_text = sentence_to_bi_grams(n_grams, tokenized_text)\n",
    "        output.append(parsed_text)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = sentences_to_bi_grams(phrases_model, papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = ' '.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'highly contagious severe cases lead acute respiratory distress multiple organ failure 3 11 march 202'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text[: 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_tokens(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[: 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tokens, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrases_model.save('phrases_model.txt')\n",
    "# phrases_model= Phraser.load('phrases_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done  0  papers in  0.0067048000055365264  seconds\n",
      "done  500  papers in  10.631782699696487  seconds\n",
      "done  1000  papers in  7.8358053001138614  seconds\n",
      "done  1500  papers in  7.038967199987383  seconds\n",
      "done  2000  papers in  6.566383399927872  seconds\n",
      "done  2500  papers in  6.56111050004256  seconds\n",
      "done  3000  papers in  6.073433299810858  seconds\n",
      "done  3500  papers in  6.77269339976192  seconds\n",
      "done  4000  papers in  9.315488700071  seconds\n",
      "done  4500  papers in  9.450686000156566  seconds\n",
      "done  5000  papers in  10.072518799919635  seconds\n",
      "done  5500  papers in  9.106466499943053  seconds\n",
      "done  6000  papers in  8.344403199997032  seconds\n",
      "done  6500  papers in  8.469056899935822  seconds\n",
      "done  7000  papers in  8.367996099987067  seconds\n",
      "done  7500  papers in  8.089393200047198  seconds\n",
      "done  8000  papers in  8.690419099802966  seconds\n",
      "done  8500  papers in  8.256924200148205  seconds\n",
      "done  9000  papers in  8.280585600063205  seconds\n",
      "done  9500  papers in  8.39931279992743  seconds\n",
      "done  10000  papers in  8.446324200049276  seconds\n",
      "done  10500  papers in  8.823179300205084  seconds\n",
      "done  11000  papers in  11.421507600010955  seconds\n",
      "done  11500  papers in  8.586531499939156  seconds\n",
      "done  12000  papers in  8.368910999881336  seconds\n",
      "done  12500  papers in  8.937019099888857  seconds\n",
      "done  13000  papers in  8.728735300101107  seconds\n",
      "done  13500  papers in  7.870640699984506  seconds\n",
      "done  14000  papers in  4.78055200005474  seconds\n",
      "done  14500  papers in  7.051583300330094  seconds\n",
      "done  15000  papers in  5.550060299938195  seconds\n",
      "done  15500  papers in  4.734978999986197  seconds\n",
      "done  16000  papers in  10.524745900009293  seconds\n",
      "done  16500  papers in  9.974496699869633  seconds\n",
      "done  17000  papers in  11.241179100135923  seconds\n",
      "done  17500  papers in  12.04310410002654  seconds\n",
      "done  18000  papers in  11.605693200210226  seconds\n",
      "done  18500  papers in  12.502399000019068  seconds\n",
      "done  19000  papers in  17.4813198998163  seconds\n",
      "done  19500  papers in  9.970866199859302  seconds\n",
      "done  20000  papers in  9.487811599829001  seconds\n",
      "done  20500  papers in  5.694571700136294  seconds\n",
      "done  21000  papers in  10.487434199880227  seconds\n",
      "done  21500  papers in  5.021971299953293  seconds\n",
      "done  22000  papers in  5.17684870006633  seconds\n",
      "done  22500  papers in  7.620084699898143  seconds\n",
      "done  23000  papers in  7.603209500244702  seconds\n",
      "done  23500  papers in  8.162166900132434  seconds\n",
      "done  24000  papers in  7.339385199986282  seconds\n",
      "done  24500  papers in  7.605203200058895  seconds\n",
      "done  25000  papers in  8.278863299987279  seconds\n",
      "done  25500  papers in  8.08199950029666  seconds\n",
      "done  26000  papers in  7.043609999949695  seconds\n",
      "done  26500  papers in  7.976853199914331  seconds\n",
      "done  27000  papers in  9.69853359994886  seconds\n",
      "done  27500  papers in  8.846947300145985  seconds\n",
      "done  28000  papers in  11.242099200127996  seconds\n",
      "done  28500  papers in  10.811627199916984  seconds\n",
      "done  29000  papers in  7.8938600001711166  seconds\n",
      "done  29500  papers in  7.293715700114262  seconds\n",
      "done  30000  papers in  7.557866599963745  seconds\n",
      "done  30500  papers in  7.756417999858968  seconds\n",
      "done  31000  papers in  7.438945500020054  seconds\n",
      "done  31500  papers in  7.392618199897697  seconds\n",
      "done  32000  papers in  8.547792299927096  seconds\n",
      "done  32500  papers in  9.035717200051295  seconds\n",
      "done  33000  papers in  8.49663480023446  seconds\n",
      "done  33500  papers in  8.570039200028987  seconds\n",
      "done  34000  papers in  7.668648699953337  seconds\n",
      "done  34500  papers in  8.77952199993888  seconds\n",
      "done  35000  papers in  11.21698339964496  seconds\n",
      "done  35500  papers in  10.670485500231734  seconds\n",
      "done  36000  papers in  10.269349899812369  seconds\n",
      "done  36500  papers in  10.761218299972825  seconds\n",
      "done  37000  papers in  8.74594099992828  seconds\n",
      "done  37500  papers in  7.792298999949708  seconds\n",
      "done  38000  papers in  10.125877800106537  seconds\n",
      "done  38500  papers in  7.803534099919489  seconds\n",
      "done  39000  papers in  10.551418299684883  seconds\n",
      "done  39500  papers in  12.892069799854653  seconds\n",
      "done  40000  papers in  13.087608399946475  seconds\n",
      "done  40500  papers in  11.376763599939295  seconds\n",
      "done  41000  papers in  11.421792900015134  seconds\n",
      "done  41500  papers in  10.044641399857937  seconds\n",
      "done  42000  papers in  11.791666599994642  seconds\n",
      "done  42500  papers in  10.977566500179819  seconds\n",
      "done  43000  papers in  11.99509970001236  seconds\n",
      "done  43500  papers in  12.26758990030794  seconds\n",
      "done  44000  papers in  23.58554870003718  seconds\n",
      "done  44500  papers in  9.582927999872481  seconds\n",
      "done  45000  papers in  7.806086799973855  seconds\n",
      "done  45500  papers in  7.803877900034422  seconds\n",
      "done  46000  papers in  6.921069099902525  seconds\n",
      "done  46500  papers in  7.307727799809072  seconds\n",
      "done  47000  papers in  7.564703300129622  seconds\n",
      "done  47500  papers in  8.364630899988697  seconds\n",
      "done  48000  papers in  8.635760899909656  seconds\n",
      "done  48500  papers in  8.677837000315776  seconds\n",
      "done  49000  papers in  8.485023100103717  seconds\n",
      "done  49500  papers in  11.108702500074287  seconds\n",
      "done  50000  papers in  9.563720199861564  seconds\n",
      "done  50500  papers in  8.556388099910691  seconds\n",
      "done  51000  papers in  11.200155999875278  seconds\n",
      "done  51500  papers in  8.247258299772511  seconds\n",
      "done  52000  papers in  9.491568199911853  seconds\n",
      "done  52500  papers in  12.546004999778233  seconds\n",
      "done  53000  papers in  10.037230300091323  seconds\n",
      "done  53500  papers in  9.352148799953284  seconds\n",
      "done  54000  papers in  10.702338700168184  seconds\n",
      "done  54500  papers in  7.736472900098306  seconds\n",
      "done  55000  papers in  8.301603700121632  seconds\n",
      "done  55500  papers in  7.777411399772973  seconds\n",
      "done  56000  papers in  4.526029999891762  seconds\n",
      "done  56500  papers in  4.810374600026989  seconds\n",
      "done  57000  papers in  6.8173803999088705  seconds\n",
      "done  57500  papers in  4.9541665000870125  seconds\n",
      "done  58000  papers in  8.111580699973274  seconds\n",
      "done  58500  papers in  2.195206899792538  seconds\n",
      "done  59000  papers in  5.730449099908583  seconds\n",
      "done  59500  papers in  3.5773805001663277  seconds\n",
      "done  60000  papers in  4.2395747999689775  seconds\n",
      "done  60500  papers in  3.401404900359921  seconds\n",
      "done  61000  papers in  8.269576999897254  seconds\n",
      "done  61500  papers in  8.480138399783755  seconds\n",
      "done  62000  papers in  16.425768599976436  seconds\n",
      "done  62500  papers in  13.50097060011467  seconds\n",
      "done  63000  papers in  7.77957010002865  seconds\n",
      "done  63500  papers in  7.240057200426236  seconds\n",
      "done  64000  papers in  13.572968499938725  seconds\n",
      "done  64500  papers in  3.7252663999970537  seconds\n",
      "done  65000  papers in  5.104924700019183  seconds\n",
      "done  65500  papers in  4.927064499992412  seconds\n",
      "done  66000  papers in  8.399027799998294  seconds\n",
      "done  66500  papers in  5.780018999881577  seconds\n",
      "done  67000  papers in  7.989298599917674  seconds\n",
      "done  67500  papers in  8.581978500267724  seconds\n",
      "done  68000  papers in  8.77495019993512  seconds\n",
      "done  68500  papers in  9.213913399726152  seconds\n",
      "done  69000  papers in  7.6193319002049975  seconds\n",
      "done  69500  papers in  5.8739454000024125  seconds\n",
      "done  70000  papers in  11.036939699828508  seconds\n",
      "done  70500  papers in  5.409244899827172  seconds\n",
      "done  71000  papers in  5.871444399817847  seconds\n"
     ]
    }
   ],
   "source": [
    "# This code creates a single text file from all the abstracts and main texts\n",
    "# Takes the list of dictionaries as input\n",
    "a = open(data_path + 'single_text_file.txt', 'a', encoding='UTF-8')\n",
    "all_text = ''\n",
    "t_time = 0\n",
    "for num, i in enumerate(lit):\n",
    "    start = time.perf_counter()\n",
    "    all_text = all_text+i['abstract']\n",
    "    all_text = all_text+i['body_text']\n",
    "    end = time.perf_counter()\n",
    "    t_time = t_time + (end-start)\n",
    "    if num%500 == 0:\n",
    "        a.write(all_text)\n",
    "        all_text = ''\n",
    "        print (\"done \", num, \" papers in \", t_time, \" seconds\")\n",
    "        t_time = 0\n",
    "        \n",
    "a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose(symptoms, diseases):\n",
    "    \"\"\"\n",
    "    Takes in list of symptoms and list of diseases (maybe make global) and produces avg similarities \n",
    "    between to each disease.\n",
    "    \n",
    "    Param: symptoms --> list\n",
    "    Param: diseases --> list\n",
    "    Output: sims --> dict{similarity: disease}\n",
    "    \"\"\"\n",
    "    sims = {}\n",
    "    for i in diseases:\n",
    "        cos_list = []\n",
    "        for j in symptoms:\n",
    "            cos_list.append(cosine(we_dict[i], we_dict[j]))\n",
    "        avg_cos = sum(cos_list)/len(cos_list)\n",
    "        sims[avg_cos] = i\n",
    "        \n",
    "    return sims\n",
    "    \n",
    "sims = diagnose(symptoms, diseases)\n",
    "top_diagnosis = sims[min(sims.keys())]\n",
    "top_5 = [sims[x] for x in sorted(sims.keys())[:5]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
